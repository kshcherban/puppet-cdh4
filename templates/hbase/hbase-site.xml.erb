<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value><%= @defaultfs_uri %></value>
        <description>The name and URI of the default FS.</description>
    </property>
    <property>
        <name>hbase.rootdir</name>
        <value><%= @defaultfs_uri %><%= @hbase_root_dir %></value>
        <description>The directory shared by region servers in HDFS</description>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.hregion.memstore.mslab.enabled</name>
        <value>true</value>
        <description>Prevent heap fragmentation under heavy write</description>
    </property>
    <property>
        <name>hbase.hregion.max.filesize</name>
        <value><%= @region_size %></value>
        <description>Maximum HStoreFile size. If any one of a column families' HStoreFiles has grown to exceed this value, the hosting HRegion is split in two. 300GB</description>
    </property>
    <property>
        <name>hbase.regionserver.region.split.policy</name>
        <value>org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy</value>
        <description>Important setting for CDH4.4  - controlls default split policy</description>
    </property>
<!--Compaction options #################### -->
    <property>
        <name>hbase.hregion.majorcompaction</name>
        <value><%= @compaction_maj_time %></value>
        <description>The time (in miliseconds) between 'major' compactions of all HStoreFiles in a region. 302400000 - 3.5 days = twice a week</description>
    </property>
    <property>
        <name>hbase.store.compaction.ratio</name>
        <value>1.4F</value>
        <description>Ratio used in compaction file selection algorithm (default 1.2f)</description>
    </property>
    <property>
        <name>hbase.hstore.compaction.max</name>
        <value><%= @compaction_min_max %></value>
        <description>Max number of HStoreFiles to compact per 'minor' compaction</description>
    </property>
    <property>
        <name>hbase.hstore.compaction.min</name>
        <value>4</value>
        <description>Minimum amount of files to compact in one minor compact. Default is 3</description>
    </property>
    <property>
        <name>hbase.hstore.compaction.min.size</name>
        <value><%= @compaction_min_size %></value>
        <description>Any StoreFile smaller than this setting with automatically be a candidate for compaction. Should be in response with hbase.hregion.memstore.flush.size</description>
    </property>
    <property>
        <name>hbase.hstore.compaction.max.size</name>
        <value><%= @compaction_maj_size %></value>
        <description>Any StoreFile larger than this setting with automatically be excluded from compaction. 5G</description>
    </property>
    <property>
        <name>hbase.hstore.useExploringCompation</name>
        <value>true</value>
        <description>smarter compaction-selection algorithm</description>
    </property>
    <property>
        <name>hbase.hstore.useExploringCompaction</name>
        <value>true</value>
        <description>smarter compaction-selection algorithm</description>
    </property>
<!-- End compaction options ############### -->
    <property>
        <name>hbase.regions.percheckin</name>
        <value>1</value>
    </property>
    <property>
        <name>hbase.regions.nobalancing.count</name>
        <value>1</value>
    </property>
    <property>
        <name>hbase.regions.close.max</name>
        <value>1</value>
    </property>
    <property>
        <name>hbase.hstore.blockingStoreFiles</name>
        <value>30</value>
        <description>If more than this number of StoreFiles in any one Store (one StoreFile is written per flush of MemStore) then updates are blocked for this HRegion until a compaction is completed, or until hbase.hstore.blockingWaitTime has been exceeded</description>
    </property>
    <property>
        <name>hfile.block.cache.size</name>
        <value>0.05</value>
        <description>Percentage of heap used for block cache on regionserver. 0 disables cache</description>
    </property>
    <property>
        <name>hbase.hregion.memstore.block.multiplier</name>
        <value>2</value>
        <description>Useful preventing runaway memstore during spikes in update traffic. Without an upper-bound, memstore fills such that when it flushes the resultant flush files take a long time to compact or split, or worse, we OOME</description>
    </property>
    <property>
        <name>hbase.rpc.timeout</name>
        <value><%= @rpc_timeout %></value>
        <description>Timeout of internal HBase RPC calls in miliseconds. Default is 60 seconds. - 30 min</description>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value><%= @zookeeper_quorum.sort.join(',') %></value>
    </property>
    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value><%= @zookeeper_datadir %></value>
    </property>
    <property>
        <name>zookeeper.znode.parent</name>
        <value><%= @zookeeper_znode_parent %></value>
    </property>
    <property>
        <name>hbase.zookeeper.property.maxClientCnxns</name>
        <value>0</value>
    </property>
    <property>
        <name>hbase.client.pause</name>
        <value>5000</value>
        <description>General client pause value. Used mostly as value to wait before running a retry of a failed get, region lookup, etc</description>
    </property>
    <property>
        <name>hbase.client.scanner.max.result.size</name>
        <value>10485760</value>
        <description>Maximum return result from region server in bytes - 10MB</description>
    </property>
    <property>
        <name>hbase.regionserver.maxlogs</name>
        <value><%= @regionserver_maxlogs %></value>
        <description>Maximum amount of WAL log files kept in memory. Default is 32 which is maybe too much in write heavy case.</description>
    </property>
    <property>
        <name>hbase.snapshot.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.regionserver.handler.count</name>
        <value><%= @regionserver_handlers %></value>
        <description>Count of RPC Server instances spun up on RegionServers</description>
    </property>
    <property>
        <name>hbase.regionserver.lease.period</name>
        <value><%= @regionserver_lease %></value>
        <description>HRegion server lease period in milliseconds - 30 min</description>
    </property>
    <property>
        <name>hbase.client.write.buffer</name>
        <value>10485760</value>
        <description>Size of the client write buffer in bytes</description>
    </property>
    <property>
        <name>hbase.client.scanner.caching</name>
        <value>100</value>
        <description>Number of rows that will be fetched when calling next on a scanner</description>
    </property>
    <property>
        <name>hbase.regionserver.nbreservationblocks</name>
        <value>2</value>
        <description>The number of reservation blocks which are used to prevent unstable region servers caused by an OOME</description>
    </property>
    <property>
        <name>hbase.hregion.memstore.flush.size</name>
        <value>1073741824</value>
        <description>Memstore will be flushed to disk if size of the memstore exceeds this number of bytes. Value is checked by a thread that runs every hbase.server.thread.wakefrequency</description>
    </property>
    <property>
        <name>hbase.regionserver.global.memstore.upperLimit</name>
        <value>0.6</value>
        <description>Maximum size of all memstores in a region server before new updates are blocked and flushes are forced. Defaults to 40% of heap</description>
    </property>
    <property>
        <name>hbase.regionserver.global.memstore.lowerLimit</name>
        <value>0.5</value>
        <description>When memstores are being forced to flush to make room in memory, keep flushing until we hit this mark. Defaults to 30% of heap. This value equal to hbase.regionserver.global.memstore.upperLimit causes the minimum possible flushing to occur when updates are blocked due to memstore limiting</description>
    </property>
    <property>
        <name>zookeeper.session.timeout</name>
        <value><%= @zookeeper_timeout %></value>
    </property>
    <property>
        <name>hbase.regionserver.logroll.period</name>
        <value><%= @regionserver_logroll %></value>
        <description>Interval in miliseconds between log roll attempts. Default is 3600000 (1 hour)</description>
    </property>
    <property>
        <name>hbase.server.versionfile.writeattempts</name>
        <value>10</value>
        <description>Amount of retries to write version file</description>
    </property>
    <property>
        <name>hbase.client.keyvalue.maxsize</name>
        <value>0</value>
    </property>
    <property>
        <name>hbase.hregion.memstore.mslab.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.hregion.memstore.mslab.chunksize</name>
        <value>2097152</value>
    </property>
    <property>
        <name>hbase.hregion.memstore.mslab.max.allocation</name>
        <value>262144</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.tickTime</name>
        <value>6000</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.initLimit</name>
        <value>10</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.syncLimit</name>
        <value>5</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value>/var/zookeeper</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.maxClientCnxns</name>
        <value>0</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.minSessionTimeout</name>
        <value>12000</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.maxSessionTimeout</name>
        <value><%= @zookeeper_timeout %></value>
    </property>
</configuration>
